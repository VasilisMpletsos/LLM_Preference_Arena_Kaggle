{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398da5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import remove_extra_brackets, CLASSIFICATION_PROMPT\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from torch.nn import Linear, BCEWithLogitsLoss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Muon, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d91db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_brackets(input: str) -> str:\n",
    "    text = input[2:-2]\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b446746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple CSV files\n",
    "df = datasets.load_dataset('csv', data_files={\n",
    "    'train': './data/train.csv',\n",
    "    'test': './data/test.csv'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3f929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "print(tokenizer.model_max_length)\n",
    "model_classification = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=3)\n",
    "model_classification = model_classification.to(\"cuda\", torch.bfloat16)\n",
    "# model_maskedLM = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3101fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_classification.longformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccd4c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 57477\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "202467ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataset(row):\n",
    "    cleaned_prompt = remove_extra_brackets(row['prompt'])\n",
    "    cleaned_response_a = remove_extra_brackets(row['response_a'])\n",
    "    cleaned_response_b = remove_extra_brackets(row['response_b'])\n",
    "    prompt = CLASSIFICATION_PROMPT.format(\n",
    "        prompt=cleaned_prompt,\n",
    "        response_a=cleaned_response_a,\n",
    "        response_b=cleaned_response_b\n",
    "    )\n",
    "    winner = [row['winner_model_a'], row['winner_model_b'], row['winner_tie']]\n",
    "    return {\n",
    "        \"final_prompt\": prompt,\n",
    "        \"winner\": winner\n",
    "    }\n",
    "    \n",
    "def tokenize_dataset(batch):\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"final_prompt\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=4096,\n",
    "        truncation=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e22be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n"
     ]
    }
   ],
   "source": [
    "df = df.map(fix_dataset, batched=False).remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b','winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "df = df.map(tokenize_dataset, batched=True, num_proc=6).remove_columns(['final_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc3119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['winner', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 57477\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['winner', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a108d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_format(\"torch\")\n",
    "train_dataloader = DataLoader(df[\"train\"], batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9de8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5ca7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification = torch.compile(model_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1797 [00:00<?, ?it/s]W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] or:\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] to include these operations in the captured graph.\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] \n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] Graph break: from user code at:\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]   File \"/venv/main/lib/python3.12/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1794, in torch_dynamo_resume_in_forward_at_1789\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]     outputs = self.longformer(\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]   File \"/venv/main/lib/python3.12/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1599, in forward\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]     encoder_outputs = self.encoder(\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]   File \"/venv/main/lib/python3.12/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1243, in forward\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0]     is_global_attn = is_index_global_attn.flatten().any().item()\n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] \n",
      "W1227 20:12:13.452000 6943 site-packages/torch/_dynamo/variables/tensor.py:1048] [1/0] \n",
      "  0%|          | 1/1797 [00:30<15:02:46, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.6610779166221619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 51/1797 [01:39<40:34,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, Loss: 0.6378701329231262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 101/1797 [02:49<39:30,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 0.6400316953659058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 151/1797 [03:59<38:30,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150, Loss: 0.6345478892326355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 157/1797 [04:08<38:18,  1.40s/it]"
     ]
    }
   ],
   "source": [
    "# optimizer = Muon(model_classification.parameters(), lr=5e-5)\n",
    "optimizer = AdamW(model_classification.parameters(), lr=5e-5)\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_classification.train()\n",
    "    total_loss = 0\n",
    "    for step, data in enumerate(tqdm(train_dataloader)):\n",
    "        data = {key: value.to(\"cuda\") for key, value in data.items()}\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model_classification(data[\"input_ids\"], attention_mask=data[\"attention_mask\"]).logits\n",
    "            loss = loss_fn(outputs, data[\"winner\"].float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87afeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21fed784",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data[\"input_ids\"] = data[\"input_ids\"].to(\"cuda\")\n",
    "    outputs = model_classification(data[\"input_ids\"]).logits\n",
    "    # predictions = torch.sigmoid(outputs)\n",
    "    # print(\"Predictions:\", predictions)\n",
    "    # print(\"Winners:\", winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54182030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1114, -0.1926, -0.0627],\n",
       "        [ 0.1262, -0.1778, -0.0078],\n",
       "        [ 0.1090, -0.1850, -0.0451],\n",
       "        [ 0.1148, -0.1850, -0.0530]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7248d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
