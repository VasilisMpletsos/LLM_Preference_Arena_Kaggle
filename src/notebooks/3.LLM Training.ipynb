{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0487422f",
   "metadata": {},
   "source": [
    "# Under Construction ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b110d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datasets\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Muon, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from transformers import DataCollatorWithPadding\n",
    "from utils import remove_extra_brackets, CLASSIFICATION_PROMPT\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442e3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba588be",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./logs/qwen_llm_finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdda2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple CSV files\n",
    "df = datasets.load_dataset('csv', data_files={\n",
    "    'train': '../data/train.csv',\n",
    "    'test': '../data/test.csv'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37554796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length is 1010000 characters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89d2d183c9e46de87192e6731fda828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-4B-Instruct-2507 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "print(f\"Model max length is {tokenizer.model_max_length} characters.\")\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\", device_map=\"cuda\", num_labels=3, quantization_config=quantization_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,349,952 || all params: 4,069,825,728 || trainable%: 1.1634\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "# WARNING: The head is not trained and so when lora is used must be set to trainable by hand\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Enable gradient checkpointing on raw pytorch\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c6f86",
   "metadata": {},
   "source": [
    "## Fix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76097a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_dataset(row):\n",
    "    cleaned_prompt = remove_extra_brackets(row['prompt'])\n",
    "    cleaned_response_a = remove_extra_brackets(row['response_a'])\n",
    "    cleaned_response_b = remove_extra_brackets(row['response_b'])\n",
    "    \n",
    "    full_prompt = CLASSIFICATION_PROMPT.format(\n",
    "        prompt=cleaned_prompt,\n",
    "        response_a=cleaned_response_a,\n",
    "        response_b=cleaned_response_b\n",
    "    )\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        full_prompt\n",
    "    )\n",
    "    \n",
    "    winner = [row['winner_model_a'], row['winner_model_b'], row['winner_tie']]\n",
    "    \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"winner\": winner,\n",
    "        \"length\": len(tokenized[\"input_ids\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d771f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.map(fix_dataset, batched=False).remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b','winner_model_a', 'winner_model_b', 'winner_tie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e19492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(lambda batch: np.array(batch[\"length\"]) <= 12000, batched=True).remove_columns([\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfdddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = df['train'].train_test_split(test_size=0.05, seed=42)\n",
    "df['train'] = train_val_split['train']\n",
    "df['validation'] = train_val_split['test']\n",
    "df = df.with_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "train_dataloader = DataLoader(df[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(df[\"validation\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df8a43",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03db552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 1/10:   0%|          | 0/719 [00:00<?, ?it/s]/venv/main/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/venv/main/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Epoch 1/10:   0%|          | 0/13644 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot handle batch sizes > 1 if no padding token is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m data = {key: value.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m data.items()}\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     37\u001b[39m         _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:921\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    920\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/modeling_layers.py:142\u001b[39m, in \u001b[36mGenericForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m     batch_size = inputs_embeds.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_size != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot handle batch sizes > 1 if no padding token is defined.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     last_non_pad_token = -\u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Cannot handle batch sizes > 1 if no padding token is defined."
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "# optimizer = bnb.optim.Adam(model.parameters(), lr=1e-4, optim_bits=32)\n",
    "EPOCHS = 10\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "GRADIENT_ACCUMULATION_STEPS = 64\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "grad_steps_corrects = 0\n",
    "grad_steps_count = 0\n",
    "\n",
    "train_size = len(train_dataloader)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_bar = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", leave=True, position=0\n",
    "    )\n",
    "    validation_bar = tqdm(\n",
    "        val_dataloader,\n",
    "        desc=f\"Validation {epoch + 1}/{EPOCHS}\",\n",
    "        leave=False,\n",
    "        position=0,\n",
    "    )\n",
    "    for step, data in enumerate(train_bar):\n",
    "        data = {key: value.to(\"cuda\") for key, value in data.items()}\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model(data[\"input_ids\"]).logits\n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, true_labels = torch.max(data[\"winner\"], 1)\n",
    "                examples_count = data[\"input_ids\"].size(0)\n",
    "                correct_count = (predicted == true_labels).sum().item()\n",
    "                grad_steps_count += examples_count\n",
    "                total_count += examples_count\n",
    "                total_correct += correct_count\n",
    "                grad_steps_corrects += correct_count\n",
    "                if (step + 1) % (GRADIENT_ACCUMULATION_STEPS // 4) == 0:\n",
    "                    train_bar.set_postfix(\n",
    "                        {\n",
    "                            \"Prediction\": f\"{predicted.cpu().tolist()} | {true_labels.cpu().tolist()}\"\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            loss = loss_fn(outputs, true_labels)\n",
    "\n",
    "        (loss / GRADIENT_ACCUMULATION_STEPS).backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            accuracy = 100 * (grad_steps_corrects / grad_steps_count)\n",
    "            grad_steps_corrects = 0\n",
    "            grad_steps_count = 0\n",
    "\n",
    "            train_bar.set_postfix({\"accuracy\": f\"{(accuracy):.2f}%\"})\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\n",
    "                \"Accuracy/train\", accuracy, (epoch * train_size) + (step + 1)\n",
    "            )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if step % 16000 == 0 and step != 0:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in validation_bar:\n",
    "                with torch.no_grad():\n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                        data = {\n",
    "                            key: value.to(\"cuda\") for key, value in data.items()\n",
    "                        }\n",
    "                        outputs = model(data[\"input_ids\"]).logits\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        _, true_labels = torch.max(data[\"winner\"], 1)\n",
    "                        total += true_labels.size(0)\n",
    "                        correct += (predicted == true_labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * (correct / total)\n",
    "            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "            writer.add_scalar(\n",
    "                \"Accuracy/validation\", accuracy, (epoch * train_size) + (step + 1)\n",
    "            )\n",
    "            model.train()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{EPOCHS}, Avg Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{EPOCHS}, Training Accuracy: {100 * (total_correct / total_count):.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
